{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateusz/.cache/pypoetry/virtualenvs/musicsae-_nVEe2b5-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import INPUT_PATH\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "EXAMPLES_LEN = 5\n",
    "BATCH_SIZE = 5\n",
    "N_TOKENS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateusz/.cache/pypoetry/virtualenvs/musicsae-_nVEe2b5-py3.11/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-small', device=DEVICE)\n",
    "model.set_generation_params(\n",
    "\tuse_sampling=True,\n",
    "\ttop_k=250,\n",
    "\tduration=EXAMPLES_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = lambda x, s: DataLoader(x, batch_size=BATCH_SIZE, shuffle=s, pin_memory=True if torch.cuda.is_available() else False)\n",
    "ds=torch.load(INPUT_PATH('8bit_encoded.pt'))[:225, :, :].cpu()\n",
    "ds = TensorDataset(ds)\n",
    "train_ds, val_ds = random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_dl, val_dl = dl(train_ds, True), dl(val_ds, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, latent_dim=64, sparsity_target=0.05, sparsity_weight=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim),\n",
    "        )\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z, self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:52<00:00,  6.52s/it, epoch: 99, loss: 8.791 val_los::2.451]\n"
     ]
    }
   ],
   "source": [
    "hook_point = model.lm.get_submodule('transformer.layers.12.cross_attention.out_proj')\n",
    "n = hook_point.in_features\n",
    "sae = LitAutoEncoder(input_dim=n, latent_dim=5 * n).to(DEVICE)\n",
    "sae_diff = []\n",
    "bottlneck = []\n",
    "\n",
    "def perform_sae(module, input, output):\n",
    "    z, out = sae(output)\n",
    "    sae_diff.append((out, output))\n",
    "    bottlneck.append(z)\n",
    "\n",
    "hook_point.register_forward_hook(perform_sae)\n",
    "\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=1e-3)\n",
    "a_coef = 1e-3\n",
    "epochs = 100\n",
    "with tqdm.tqdm(total=epochs) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "        sae_diff, bottlneck, total_loss = [], [], 0\n",
    "        for batch in train_dl:\n",
    "            batch = batch[0].to(DEVICE)\n",
    "            with model.autocast:\n",
    "                model.lm.compute_predictions(\n",
    "                    batch, [ConditioningAttributes(text={\"description\": \"Amazing metal music\"})]\n",
    "                )\n",
    "            loss = torch.norm(sae_diff[-1][0]-sae_diff[-1][1]) + a_coef*torch.norm(bottlneck[-1], p=1)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            sae_diff, bottlneck, val_loss = [], [], 0\n",
    "            for batch in val_dl:\n",
    "                # batch = batch['encoded_music'].to(DEVICE)\n",
    "                batch = batch[0].to(DEVICE)\n",
    "                with model.autocast:\n",
    "                    model.lm.compute_predictions(\n",
    "                        batch, [ConditioningAttributes(text={\"description\": \"Amazing metal music\"})]\n",
    "                    )\n",
    "                val_loss += torch.norm(sae_diff[-1][0]-sae_diff[-1][1]) + a_coef*torch.norm(bottlneck[-1], p=1).item()\n",
    "                \n",
    "        pbar.set_postfix_str(f'epoch: {epoch}, loss: {total_loss:.3f} val_los::{val_loss:.3f}')\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicsae-_nVEe2b5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
