{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mszawerda/.cache/pypoetry/virtualenvs/musicsae-j7RS21dY-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device (rank 0): cuda\n"
     ]
    }
   ],
   "source": [
    "from musicgen.model.musicgen import MusicGen\n",
    "import torch.distributed as dist\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from musicgen.utils.torch_utils import print_once, get_rank, get_world_size\n",
    "from utils import INPUT_PATH\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "EXAMPLES_LEN = 5\n",
    "BATCH_SIZE = 5\n",
    "N_TOKENS = 5\n",
    "dist.init_process_group(backend=\"gloo\", init_method=\"file:///tmp/sharedfile\", rank=0, world_size=1)\n",
    "use_gpu = dist.is_available()\n",
    "\n",
    "amp_type = 'fp16' if use_gpu else 'fp32'\n",
    "accelerator = Accelerator(mixed_precision=amp_type)\n",
    "DEVICE = accelerator.device\n",
    "rank = get_rank()\n",
    "world_size = get_world_size()\n",
    "print(f'Device (rank {rank}): {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>=>=> Loading an LM and conditioner model : [facebook/musicgen-small]\n",
      "WARNING : Audiocraft version is fixed to 1.2.0 while loading pretrained models from HuggingfaceHub.\n",
      "[Unmatched keywards]\n",
      "\tcondition_provider.conditioners.description.output_proj.weight\n",
      "\tcondition_provider.conditioners.description.output_proj.bias\n",
      "=>=>=> Loading a compression model : [facebook/musicgen-small]\n",
      "WARNING : Audiocraft version is fixed to 1.2.0 while loading pretrained models from HuggingfaceHub.\n"
     ]
    }
   ],
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-small', device=DEVICE)\n",
    "model.set_generation_params(\n",
    "\tuse_sampling=True,\n",
    "\ttop_k=250,\n",
    "\tduration=EXAMPLES_LEN\n",
    ")\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->-> Searching audio files...\n",
      "->-> Found 2 files.\n",
      "->-> Loading audio metadata...\n",
      "->-> Keep 2 files.\n"
     ]
    }
   ],
   "source": [
    "from musicgen.data.audio_dataset import MonoAudioFilesDataset\n",
    "ds = MonoAudioFilesDataset('/home/mszawerda/music-sae/dependencies/musicgen/example/dataset/audio')\n",
    "dl = lambda x, s: DataLoader(x, batch_size=BATCH_SIZE, shuffle=s, pin_memory=True if torch.cuda.is_available() else False)\n",
    "train_dl, val_dl = dl(ds, True), dl(ds, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl = lambda x, s: DataLoader(x, batch_size=BATCH_SIZE, shuffle=s, pin_memory=True if torch.cuda.is_available() else False)\n",
    "# ds=torch.load(INPUT_PATH('8bit_encoded.pt'))[:225, :, :].cpu()\n",
    "# ds = TensorDataset(ds)\n",
    "# train_ds, val_ds = random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "# train_dl, val_dl = dl(train_ds, True), dl(val_ds, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, latent_dim=64, sparsity_target=0.05, sparsity_weight=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim),\n",
    "        )\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z, self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.91it/s, epoch: 99, loss: 142.841 val_los::142.841]\n"
     ]
    }
   ],
   "source": [
    "hook_point = model.lm.get_submodule('transformer.layers.12.cross_attention.out_proj')\n",
    "n = hook_point.in_features\n",
    "sae = LitAutoEncoder(input_dim=n, latent_dim=5 * n).to(DEVICE)\n",
    "sae_diff = []\n",
    "bottlneck = []\n",
    "\n",
    "def perform_sae(module, input, output):\n",
    "    z, out = sae(output)\n",
    "    sae_diff.append((out, output))\n",
    "    bottlneck.append(z)\n",
    "\n",
    "hook_point.register_forward_hook(perform_sae)\n",
    "\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=1e-3)\n",
    "a_coef = 1e-3\n",
    "epochs = 100\n",
    "with accelerator.autocast():\n",
    "    model.eval()\n",
    "    conds = model.conditioner({\"text_prompt\": [\"hello\"]*2})\n",
    "    conds.requite_grad = True\n",
    "\n",
    "loss_func = lambda x, z: torch.norm(torch.nan_to_num(x[-1][0], 0.0)-torch.nan_to_num(x[-1][1], 0.0)) + a_coef*torch.norm(torch.nan_to_num(z[-1], 0.0), p=1).item()\n",
    "with tqdm.tqdm(total=epochs) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        sae_diff, bottlneck, total_loss = [], [], 0\n",
    "        for batch in train_dl:\n",
    "            music = batch[0].to(DEVICE)\n",
    "            with torch.no_grad(), accelerator.autocast():\n",
    "                codec, _ = model.compression_model.encode(music)\n",
    "            with accelerator.autocast():\n",
    "                model.lm.compute_predictions(\n",
    "                    codec, conds.detach()\n",
    "                )\n",
    "            loss = loss_func(sae_diff, bottlneck)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            sae_diff, bottlneck, val_loss = [], [], 0\n",
    "            for batch in val_dl:\n",
    "                batch = batch[0].to(DEVICE)\n",
    "                with torch.no_grad(), accelerator.autocast():\n",
    "                    codec, _ = model.compression_model.encode(music)\n",
    "                with accelerator.autocast():\n",
    "                    model.lm.compute_predictions(\n",
    "                        codec, conds\n",
    "                    )\n",
    "                val_loss += loss_func(sae_diff, bottlneck)\n",
    "                \n",
    "        pbar.set_postfix_str(f'epoch: {epoch}, loss: {total_loss:.3f} val_los::{val_loss:.3f}')\n",
    "        pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicsae-j7RS21dY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
