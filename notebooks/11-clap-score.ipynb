{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import INPUT_DATA_DIR\n",
    "import torchaudio\n",
    "import json\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from typing import List, Dict\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name = \"laion/clap-htsat-fused\"\n",
    "max_tokens = 200\n",
    "base_dir = INPUT_DATA_DIR / \"music-bench\" / \"datashare-instruments\"\n",
    "model_sr = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ClapProcessor.from_pretrained(model_name)\n",
    "model = ClapModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_DATA_DIR / \"interp\" / \"features_grouped.json\", \"r\") as fh:\n",
    "    feat = json.load(fh)\n",
    "with open(INPUT_DATA_DIR / \"interp\" / \"final_descriptions.json\", \"r\") as fh:\n",
    "    desc = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(base_dir, location, model_sr):\n",
    "    audio_tensor, sr = torchaudio.load(str(base_dir / location).replace(\".wav\", \".mp3\"))\n",
    "    transform = torchaudio.transforms.Resample(sr, model_sr)\n",
    "    return transform(audio_tensor).numpy()[0]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_text(processor: ClapProcessor, model: ClapModel, text: str, device: torch.device):\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    emb = model.get_text_features(**inputs)\n",
    "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    return emb.squeeze(0).cpu()  # (D,)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_audios(\n",
    "    processor: ClapProcessor,\n",
    "    model: ClapModel,\n",
    "    audio_tensors: List[torch.Tensor],\n",
    "    device: torch.device,\n",
    "    batch_size: int,\n",
    "):\n",
    "    embs = []\n",
    "    for i in range(0, len(audio_tensors), batch_size):\n",
    "        batch = audio_tensors[i : i + batch_size]\n",
    "        inputs = processor(audios=batch, sampling_rate=48_000, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        a_emb = model.get_audio_features(**inputs)\n",
    "        a_emb = a_emb / a_emb.norm(dim=-1, keepdim=True)\n",
    "        embs.append(a_emb.cpu())\n",
    "    return torch.cat(embs, dim=0)\n",
    "\n",
    "\n",
    "results: Dict[str, float] = {}\n",
    "for key, details in desc.items():\n",
    "    text_emb = embed_text(processor, model, details[\"overall_summary\"], device)\n",
    "\n",
    "    audios = [load_audio(base_dir, p, 32000) for p in feat[key]]\n",
    "    audio_embs = embed_audios(processor, model, audios, device, 10)\n",
    "\n",
    "    results[key] = (audio_embs @ text_emb).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_DATA_DIR / \"interp\" / \"features_ranked.json\", \"w\") as fh:\n",
    "    json.dump(results, fh, indent=4)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
