{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionary_learning.trainers.top_k import AutoEncoderTopK\n",
    "from musicsae.nnsight_model import MusicGenLanguageModel, AutoProcessor\n",
    "import torch\n",
    "from utils import MODELS_DIR, OUTPUT_DATA_DIR, INPUT_DATA_DIR\n",
    "import torchaudio\n",
    "import nnsight\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name = \"facebook/musicgen-medium\"\n",
    "max_tokens = 200\n",
    "base_dir = INPUT_DATA_DIR / \"music-bench\" / \"datashare-instruments\"\n",
    "model_sr = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = MusicGenLanguageModel(model_name, device_map=device)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "ae = AutoEncoderTopK.from_pretrained(\n",
    "    MODELS_DIR / \"medium-sae-trivial-medium-sae-ee3b\" / \"16\" / \"trainer_0\" / \"checkpoints\" / \"ae_71100.pt\"\n",
    ").to(device)\n",
    "layer = nn_model.decoder.model.decoder.layers[16]\n",
    "ds = load_dataset(\"amaai-lab/MusicBench\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity():\n",
    "    batch = next(ds.iter(10))\n",
    "\n",
    "    def forward_audio(batch):\n",
    "        inputs = processor(\n",
    "            audio=batch[\"audio_tensor\"],\n",
    "            sampling_rate=32000,\n",
    "            text=batch[\"main_caption\"],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            with nn_model.trace(inputs, invoker_args={\"truncation\": True, \"max_length\": max_tokens}):\n",
    "                return layer.output[0].save()\n",
    "\n",
    "    act = forward_audio(batch)\n",
    "    z = ae.encode(act)\n",
    "    z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "THETA_MIN = 0.01  # ri lower bound (exclusive)\n",
    "THETA_MAX = 0.25  # ri upper bound (inclusive)\n",
    "ACT_THRESHOLD = 0.0  # τ – any mean activation > 0 counts as “present”\n",
    "TOP_K_EXAMPLES = 10\n",
    "\n",
    "\n",
    "def compute_mean_activation(z: torch.Tensor) -> torch.Tensor:\n",
    "    return z.mean(dim=1)  # average over time dimension T\n",
    "\n",
    "\n",
    "def update_corpus_statistics(\n",
    "    batch_mean_act: torch.Tensor,\n",
    "    track_ids: torch.Tensor,\n",
    "    sum_delta: torch.Tensor,\n",
    "    example_scores: Dict[int, List[float]],\n",
    "    example_ids: Dict[int, List[int]],\n",
    "    active_tracks: Dict[int, set],\n",
    "):\n",
    "    \"\"\"Accumulate global stats and keep example + track‑lists.\n",
    "\n",
    "    Args:\n",
    "        batch_mean_act: (B, F) mean activations for current batch.\n",
    "        track_ids:       (B,) global ids for each track in batch.\n",
    "        sum_delta:       (F,) running count of tracks where feature is active.\n",
    "        example_scores/example_ids: top‑K maintenance buffers.\n",
    "        active_tracks:   mapping feature → *set* of all track ids where feature active.\n",
    "    \"\"\"\n",
    "    # δ_{i,j} indicator mask: 1 if mean act > τ\n",
    "    delta = batch_mean_act > ACT_THRESHOLD\n",
    "    sum_delta += delta.float().sum(dim=0)\n",
    "\n",
    "    # For each track collect active features once (vectorised)\n",
    "    B, F = batch_mean_act.shape\n",
    "    for b in range(B):\n",
    "        tid = int(track_ids[b])\n",
    "        row = batch_mean_act[b]\n",
    "        act_mask = row > ACT_THRESHOLD\n",
    "        # Keep set of *all* active tracks per feature\n",
    "        active_idx = torch.nonzero(act_mask, as_tuple=False).flatten().tolist()\n",
    "        for feat in active_idx:\n",
    "            active_tracks.setdefault(feat, set()).add(tid)\n",
    "\n",
    "        # Maintain top‑K heaps\n",
    "        scores_sorted = row.topk(min(TOP_K_EXAMPLES, F)).indices.tolist()\n",
    "        for feat in scores_sorted:\n",
    "            score = float(row[feat])\n",
    "            if score <= ACT_THRESHOLD:\n",
    "                continue\n",
    "            buf_scores = example_scores.setdefault(feat, [])\n",
    "            buf_ids = example_ids.setdefault(feat, [])\n",
    "            insert_pos = next((i for i, s in enumerate(buf_scores) if score > s), len(buf_scores))\n",
    "            buf_scores.insert(insert_pos, score)\n",
    "            buf_ids.insert(insert_pos, tid)\n",
    "            if len(buf_scores) > TOP_K_EXAMPLES:\n",
    "                buf_scores.pop()\n",
    "                buf_ids.pop()\n",
    "\n",
    "\n",
    "def process_batch(batch, base_dir: Path, model_sr: int):\n",
    "    def load_audio(base_dir, location, model_sr):\n",
    "        audio_tensor, sr = torchaudio.load(str(base_dir / location).replace(\".wav\", \".mp3\"))\n",
    "        transform = torchaudio.transforms.Resample(sr, model_sr)\n",
    "        return transform(audio_tensor).numpy()[0]\n",
    "\n",
    "    audio_tensor = []\n",
    "    caption = []\n",
    "    location = []\n",
    "    for row, cap in zip(batch[\"location\"], batch[\"main_caption\"]):\n",
    "        if \"data_aug2\" in row:\n",
    "            continue\n",
    "        try:\n",
    "            audio_tensor.append(load_audio(base_dir, row, model_sr))\n",
    "        except Exception:\n",
    "            continue\n",
    "        caption.append(cap)\n",
    "        location.append(row)\n",
    "    return {\"main_caption\": caption, \"audio_tensor\": audio_tensor, \"location\": location}\n",
    "\n",
    "\n",
    "def analyse_dataset(\n",
    "    ds,\n",
    "    processor,\n",
    "    nn_model,\n",
    "    layer,\n",
    "    ae,\n",
    "    batch_size: int = 10,\n",
    "    max_tracks: int = 100,\n",
    "    max_tokens: int = 1024,\n",
    "    device: str | torch.device = \"cuda\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str], Dict[str, List[int]]]:\n",
    "    \"\"\"Full end‑to‑end analysis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_df         : (tracks × features) table of µ_{i,j} (may be huge!)\n",
    "    corpus_df       : per‑feature table with activation rate & keep flag\n",
    "    kept_features   : list[str] feature names kept after filtering\n",
    "    tracks_per_feat : mapping kept feature → *list* of ALL track ids where it is active\n",
    "    \"\"\"\n",
    "    num_features: int = ae.encoder.out_features\n",
    "    # Running aggregates\n",
    "    sum_delta = torch.zeros(num_features, dtype=torch.float32, device=device)\n",
    "    example_scores: Dict[int, List[float]] = {}\n",
    "    example_ids: Dict[int, List[int]] = {}\n",
    "    active_tracks: Dict[int, set] = {}\n",
    "    track_id_to_loc: Dict[int, str] = {}\n",
    "    mean_rows = []  # will collect tensors row‑wise → concatenate\n",
    "    mean_index = []\n",
    "\n",
    "    iterator = ds.iter(batch_size)\n",
    "    global_track_id = 0\n",
    "    processed_tracks = 0\n",
    "    for final_batch in tqdm(iterator, desc=\"Analysing dataset\"):\n",
    "        batch = process_batch(final_batch, base_dir, model_sr)\n",
    "        B = len(batch[\"audio_tensor\"])\n",
    "        if B <= 0:\n",
    "            continue\n",
    "        track_ids = torch.arange(global_track_id, global_track_id + B)\n",
    "        global_track_id += B\n",
    "        for i in range(B):\n",
    "            track_id_to_loc[int(track_ids[i])] = batch[\"location\"][i]\n",
    "        # Forward\n",
    "        inputs = processor(\n",
    "            audio=batch[\"audio_tensor\"],\n",
    "            sampling_rate=model_sr,\n",
    "            text=batch[\"main_caption\"],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with nn_model.trace(inputs, invoker_args={\"truncation\": True, \"max_length\": max_tokens}):\n",
    "                act = layer.output[0].save()\n",
    "            z = ae.encode(act)  # (B, T, F)\n",
    "\n",
    "        batch_mean_act = compute_mean_activation(z)  # (B, F)\n",
    "        mean_rows.append(batch_mean_act.cpu())\n",
    "        mean_index.extend(track_ids.tolist())\n",
    "\n",
    "        update_corpus_statistics(batch_mean_act, track_ids, sum_delta, example_scores, example_ids, active_tracks)\n",
    "\n",
    "        processed_tracks += B\n",
    "        if processed_tracks > max_tracks:\n",
    "            break\n",
    "\n",
    "    # ── Assemble µ_{i,j} big matrix\n",
    "    mean_tensor = torch.cat(mean_rows, dim=0)\n",
    "    feature_cols = [f\"f{idx:04d}\" for idx in range(num_features)]\n",
    "    mean_df = pd.DataFrame(mean_tensor.numpy(), index=mean_index, columns=feature_cols)\n",
    "\n",
    "    # ── Corpus‑level activation rate r_i\n",
    "    n_tracks = len(mean_df)\n",
    "    r_i = (sum_delta.detach().cpu() / n_tracks).numpy()\n",
    "\n",
    "    corpus_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": feature_cols,\n",
    "            \"activation_rate\": r_i,\n",
    "        }\n",
    "    )\n",
    "    corpus_df[\"kept\"] = (corpus_df.activation_rate > THETA_MIN) & (corpus_df.activation_rate <= THETA_MAX)\n",
    "\n",
    "    kept_features = corpus_df[corpus_df.kept].feature.tolist()\n",
    "\n",
    "    # ── Build mapping: kept feature → ALL active track ids\n",
    "    tracks_per_feat: Dict[str, List[str]] = {}\n",
    "    for feat_idx, track_set in active_tracks.items():\n",
    "        feat_name = feature_cols[feat_idx]\n",
    "        if feat_name not in kept_features:\n",
    "            continue\n",
    "        top_ids = mean_df.loc[list(track_set), feat_name].nlargest(TOP_K_EXAMPLES).index.tolist()\n",
    "        tracks_per_feat[feat_name] = [track_id_to_loc[tid] for tid in top_ids]\n",
    "\n",
    "    return mean_df, corpus_df, kept_features, tracks_per_feat\n",
    "\n",
    "\n",
    "feature_stats, corpus_df, kept_features, example_dict = analyse_dataset(\n",
    "    ds, processor, nn_model, layer, ae, batch_size=15, max_tracks=10000, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_DATA_DIR / \"interp\" / \"features.json\", \"w\") as fh:\n",
    "    json.dump({k: list(set(v)) for k, v in example_dict.items()}, fh, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "with open(INPUT_DATA_DIR / \"interp\" / \"features.json\", \"r\") as fh:\n",
    "    feat = json.load(fh)\n",
    "\n",
    "key = \"f1595\"\n",
    "for p in set(feat[key]):\n",
    "    display(Audio(str(Path(str(base_dir).replace(\"-instruments\", \"\")) / p)))\n",
    "    # display(Audio(str(base_dir / p).replace('.wav', '.mp3')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Prominent use of string instruments like violins, guitars, and other stringed instruments, often with melodic roles.\"\n",
    "# prompt = \"Lively Scottish bagpipe tune in G mixolydian mode, featuring a continuous drone, traditional grace notes, and a strong, march-like rhythm\"\n",
    "prompt = \"This is an alternative rock song with slow tempo and guitar and drums\"\n",
    "tokens = 255\n",
    "n = 5\n",
    "# set_seed(42)\n",
    "with nn_model.generate([prompt] * n, max_new_tokens=tokens):\n",
    "    outputs = nnsight.list().save()  # Initialize & .save() nnsight list\n",
    "    act = nnsight.list().save()\n",
    "    for i in range(tokens):\n",
    "        # set_seed(42)\n",
    "        if i % 5 == 0:\n",
    "            z = ae.encode(layer.output[0][:], use_threshold=True)\n",
    "            z[:, :, 6140] = -9\n",
    "            # act.append(z[:, :, 4881].detach().clone().cpu())\n",
    "            for f in [367, 1145, 1129, 3444, 1911, 6140, 5775, 1556]:\n",
    "                # z[:,:,f]=0.5\n",
    "                ...\n",
    "            layer.output[0][:] = ae.decode(z)\n",
    "        outputs.append(nn_model.generator.output)\n",
    "        nn_model.next()\n",
    "for i in range(n):\n",
    "    torchaudio.save(\n",
    "        OUTPUT_DATA_DIR / \"musicgen-sae\" / f\"out_{i}.wav\",\n",
    "        src=outputs[0][i].detach().cpu(),\n",
    "        sample_rate=nn_model.config.sampling_rate,\n",
    "        channels_first=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(act)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
