{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:29:06,409 WARNING The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "2025-05-23 15:29:06,411 INFO Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 1: 404791296 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ff8b030c5b4443949ee216aa03b720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:29:07,608 WARNING Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n",
    "\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `WhisperFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' The low quality recording features a male vocal singing over a very loud crowd clapping and cheering, while some of them are making scary sounds. It sounds like a live performance and it is very energetic.',\n",
       " ' The low quality recording features a male vocal singing over a very loud crowd clapping and cheering, while some of them are making scary sounds. It sounds like a live performance and it is very energetic.',\n",
       " ' The low quality recording features a male vocal singing over a very loud crowd clapping and cheering, while some of them are making scary sounds. It sounds like a live performance and it is very energetic.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"<|audio_bos|><|AUDIO|><|audio_eos|>Describe the music fragment in detail:\"\n",
    "audio, sr = torchaudio.load(\"/home/mszawerda/music-sae/data/input/music-bench/datashare/data/_KYo_89lgf0.wav\")\n",
    "transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=processor.feature_extractor.sampling_rate)\n",
    "audio = transform(audio)\n",
    "inputs = processor(text=[prompt] * 3, audios=[audio[0].numpy()] * 3, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**{k: v.to(\"cuda\") for k, v in inputs.items()}, max_length=300)\n",
    "generated_ids = generated_ids[:, inputs.input_ids.size(1) :]\n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
